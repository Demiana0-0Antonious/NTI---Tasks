{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_P8q1al8WsOv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid -> puts number in range between 0 and 1"
      ],
      "metadata": {
        "id": "MDNsVIJNYhWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "GhJJvJmHWtPD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid Derivative ->\n",
        "We need the derivative for backpropagation."
      ],
      "metadata": {
        "id": "zEHRuq-VY2Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(a):\n",
        "    return a * (1 - a)"
      ],
      "metadata": {
        "id": "Y3H9kzxwWtLh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU (Rectified Linear Unit) ->\n",
        "ReLU keeps positive numbers and replaces negatives with 0 to avoid some problems sigmoid has"
      ],
      "metadata": {
        "id": "QtEYw4DPZJbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    return np.maximum(0, z)"
      ],
      "metadata": {
        "id": "CNK9g8ocWtIu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU Derivative ->\n",
        "The derivative is 1 for positive values and 0 for negative values."
      ],
      "metadata": {
        "id": "Xb4P_FmbZaz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)"
      ],
      "metadata": {
        "id": "lpg50ntiWtGI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Parameters"
      ],
      "metadata": {
        "id": "j0pR1bHZZjl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We randomly set initial weights and set biases to zero.\n",
        "\n",
        "W1 → connects input layer → hidden layer\n",
        "\n",
        "b1 → bias for hidden layer\n",
        "\n",
        "W2 → connects hidden layer → output layer\n",
        "\n",
        "b2 → bias for output layer"
      ],
      "metadata": {
        "id": "EnHGOEE1ZkJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params(input_dim, hidden_dim, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
        "    b1 = np.zeros((1, hidden_dim))\n",
        "    W2 = np.random.randn(hidden_dim, 1) * 0.01\n",
        "    b2 = np.zeros((1, 1))\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "GQXPyfr0WtDe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward Propagation"
      ],
      "metadata": {
        "id": "usxEOWyfaCY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We feed input data through the network to get predictions.\n",
        "\n",
        "Z1 = X·W1 + b1\n",
        "\n",
        "A1 = ReLU(Z1) → hidden layer activation\n",
        "\n",
        "Z2 = A1·W2 + b2\n",
        "\n",
        "A2 = Sigmoid(Z2) → output layer activation (probability)"
      ],
      "metadata": {
        "id": "0OrRrDsIaH2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    return Z1, A1, Z2, A2"
      ],
      "metadata": {
        "id": "D62pjd0qaGxl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function — Binary Cross Entropy ->\n",
        "Measures how well our predictions match the labels."
      ],
      "metadata": {
        "id": "3hSu2-rOaaIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss = −\n",
        "m\n",
        "1\n",
        "​\n",
        " ∑[ylog(A2)+(1−y)log(1−A2)]"
      ],
      "metadata": {
        "id": "OQXY9EC1ahtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(y, A2):\n",
        "    m = y.shape[0]\n",
        "    loss = -np.mean(y * np.log(A2 + 1e-8) + (1 - y) * np.log(1 - A2 + 1e-8))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "CcbtcxOVWtAu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We add 1e-8 to avoid log(0) errors**"
      ],
      "metadata": {
        "id": "FptgVun9a3kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backward Propagation"
      ],
      "metadata": {
        "id": "Y7-rnIfAa_Sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we find gradients to know how to update weights."
      ],
      "metadata": {
        "id": "cZg8Njo_bG6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output layer error:\n",
        "\n",
        "𝑑\n",
        "𝑍\n",
        "2\n",
        "=\n",
        "𝐴\n",
        "2\n",
        "−\n",
        "𝑦\n",
        "dZ2=A2−y\n",
        "\n",
        "\n",
        "Then:\n",
        "\n",
        "𝑑\n",
        "𝑊\n",
        "2\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "𝐴\n",
        "1\n",
        "𝑇\n",
        "⋅\n",
        "𝑑\n",
        "𝑍\n",
        "2\n",
        "dW2=\n",
        "m\n",
        "1\n",
        "​\n",
        " A1\n",
        "T\n",
        " ⋅dZ2\n",
        "𝑑\n",
        "𝑏\n",
        "2\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑑\n",
        "𝑍\n",
        "2\n",
        "\n",
        "\n",
        "db2=\n",
        "m\n",
        "1\n",
        "​\n",
        " ∑dZ2\n",
        "\n",
        "\n",
        "Hidden layer error:\n",
        "\n",
        "𝑑\n",
        "𝑍\n",
        "1\n",
        "=\n",
        "(\n",
        "𝑑\n",
        "𝑍\n",
        "2\n",
        "⋅\n",
        "𝑊\n",
        "2\n",
        "𝑇\n",
        ")\n",
        "⋅\n",
        "𝑅\n",
        "𝑒\n",
        "𝐿\n",
        "𝑈\n",
        "′\n",
        "(\n",
        "𝑍\n",
        "1\n",
        ")\n",
        "dZ1=(dZ2⋅W2\n",
        "T\n",
        " )⋅ReLU\n",
        "′\n",
        " (Z1)\n",
        "\n",
        "\n",
        "𝑑\n",
        "𝑊\n",
        "1\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "𝑋\n",
        "𝑇\n",
        "⋅\n",
        "𝑑\n",
        "𝑍\n",
        "1\n",
        "dW1=\n",
        "m\n",
        "1\n",
        "​\n",
        " X\n",
        "T\n",
        " ⋅dZ1\n",
        "𝑑\n",
        "𝑏\n",
        "1\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑑\n",
        "𝑍\n",
        "1\n",
        "\n",
        "\n",
        "db1=\n",
        "m\n",
        "1\n",
        "​\n",
        " ∑dZ1"
      ],
      "metadata": {
        "id": "fb1Jj8IGbR0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(X, y, Z1, A1, A2, W2):\n",
        "    m = X.shape[0]\n",
        "    dZ2 = A2 - y\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "    dZ1 = np.dot(dZ2, W2.T) * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "    return dW1, db1, dW2, db2"
      ],
      "metadata": {
        "id": "5uTxGlZZZg1k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Update Parameters (Gradient Descent)**"
      ],
      "metadata": {
        "id": "gpHGy4apcHK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We move weights against the gradient to minimize loss."
      ],
      "metadata": {
        "id": "EY8vQtdwcG7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
        "    W1 -= lr * dW1\n",
        "    b1 -= lr * db1\n",
        "    W2 -= lr * dW2\n",
        "    b2 -= lr * db2\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "SYtdVMHVZgyH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop** ->\n",
        "We connect all pieces to train our network."
      ],
      "metadata": {
        "id": "czPrOvP1cb2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Toy dataset\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Generate data\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "# Initialize parameters\n",
        "W1, b1, W2, b2 = init_params(input_dim=2, hidden_dim=4)\n",
        "\n",
        "# Training\n",
        "epochs = 1000\n",
        "lr = 0.1\n",
        "\n",
        "for i in range(epochs):\n",
        "    # Forward\n",
        "    Z1, A1, Z2, A2 = forward(X, W1, b1, W2, b2)\n",
        "\n",
        "    # Loss\n",
        "    loss = compute_loss(y, A2)\n",
        "\n",
        "    # Backward\n",
        "    dW1, db1, dW2, db2 = backward(X, y, Z1, A1, A2, W2)\n",
        "\n",
        "    # Update\n",
        "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, lr)\n",
        "\n",
        "    # Print progress\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Epoch {i}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyIYgN3ocbg5",
        "outputId": "adad7569-ea42-4c3c-a51a-3214e85f20fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6932\n",
            "Epoch 100, Loss: 0.6925\n",
            "Epoch 200, Loss: 0.6542\n",
            "Epoch 300, Loss: 0.3816\n",
            "Epoch 400, Loss: 0.3235\n",
            "Epoch 500, Loss: 0.3145\n",
            "Epoch 600, Loss: 0.3127\n",
            "Epoch 700, Loss: 0.3121\n",
            "Epoch 800, Loss: 0.3119\n",
            "Epoch 900, Loss: 0.3117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**END  ;)**"
      ],
      "metadata": {
        "id": "KRXfVp-Kc22s"
      }
    }
  ]
}